{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5fb0b379",
   "metadata": {},
   "source": [
    "# 5. Clustering\n",
    "\n",
    "From the previously-obtained features, we can now cluster the data. We will use the K-means algorithm, which is a simple and efficient algorithm for clustering. Depending on the task at hand, other algorithms may be more appropriate. Please\n",
    "follow the [scikit-learn documentation](https://scikit-learn.org/stable/modules/clustering.html) for more information. \n",
    "\n",
    "Note that we use directly the features extracted with the inependent component analysis (ICA) as input for the clustering algorithm. This is not mandatory, and you can use the features extracted with the scattering network instead. We here opt for the ICA features because they lie in a lower-dimensional space, which makes the clustering algorithm more efficient (see the comments on the curse of dimensionality).\n",
    "\n",
    "Made in 2022 by Léonard Seydoux and René Steinmann."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "08f097fa",
   "metadata": {},
   "source": [
    "This notebook uses the __matplotlib__, __scikit-learn__ and __obspy__ library, please run the cell below if the packages are not installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5e6bf3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a95975",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "import obspy\n",
    "from scipy import signal\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "plt.rcParams[\"date.converter\"] = \"concise\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1f86a5c2",
   "metadata": {},
   "source": [
    "## Load features\n",
    "\n",
    "We first load the features calculated in the notebook `3_dimensionality_reduction.ipynb`. We will use the features extracted with the independent component analysis (ICA) as input for the clustering algorithm. This is not mandatory, and you can use the features extracted with the scattering network instead. We here opt for the ICA features because they lie in a lower-dimensional space, which makes the clustering algorithm more efficient (see the comments on the curse of dimensionality).\n",
    "\n",
    "> Note: since we saved the times as Python datetime objects, we need to set the `allow_pickle` flag to `True` when loading the data, to allow for object loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07c3d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load features and datetimes from file\n",
    "with np.load(\"../example/independent_components.npz\", allow_pickle=True) as data:\n",
    "    features = data[\"features\"]\n",
    "    times = data[\"times\"]\n",
    "\n",
    "# Load network\n",
    "network = pickle.load(open(\"../example/scattering_network.pickle\", \"rb\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "07564103",
   "metadata": {},
   "source": [
    "## $k$-means clustering\n",
    "\n",
    "We now perform the clustering. We will use the $k$-means algorithm, which is a simple and efficient algorithm for clustering. Depending on the task at hand, other algorithms may be more appropriate. Please follow the [scikit-learn documentation](https://scikit-learn.org/stable/modules/clustering.html) for more information.\n",
    "\n",
    "The $k$-means algorithm solves the following optimization problem:\n",
    "\n",
    "$$ \\min_{\\mathbf{C}, \\mathbf{Z}} \\sum_{i=1}^N \\sum_{k=1}^K z_{ik} \\| \\mathbf{x}_i - \\mathbf{c}_k \\|^2 $$\n",
    "\n",
    "where $\\mathbf{C} = \\{\\mathbf{c}_1, \\ldots, \\mathbf{c}_K\\}$ is the set of cluster centers, $\\mathbf{Z} = \\{z_{1}, \\ldots, z_{N}\\}$ is the set of assignment vectors, and $z_{ik} = 1$ if $\\mathbf{x}_i$ is assigned to cluster $k$ and $z_{ik} = 0$ otherwise. The assignment vectors are also called responsibilities.\n",
    "\n",
    "The $k$-means algorithm requires the number of clusters as input. Several methods exist to determine this number. Here, we will define this number  manually, and allow the user to change it. Depending on the application, we chose the number of clusters differently. \n",
    "\n",
    "### Optimize model\n",
    "\n",
    "We first instantiate the model, and optimize it. We define the number of clusters arbitrarily to 10. We then fit the model to the data, and predict the cluster labels for each sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0f691c",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_CLUSTERS = 10\n",
    "\n",
    "# Perform clustering\n",
    "model = KMeans(n_clusters=N_CLUSTERS, n_init=\"auto\", random_state=4)\n",
    "model.fit(features)\n",
    "\n",
    "# Predict cluster for each sample\n",
    "predictions = model.predict(features)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2a2d6062",
   "metadata": {},
   "source": [
    "### Vizualize cluster-wise detection rate\n",
    "\n",
    "A way to vizualize the clustering results is to plot the cluster-wise detection rate as a function of the cluster number. This is done by computing the detection curve for each cluster (one-hot encoding), and then averaging the results in a sliding window with a given smoothing kernel of $N$ points.\n",
    "\n",
    "The detection rate of a given cluster is given by the one-hot encoding of the\n",
    "cluster label as a function of time. One can smooth the detection rate by\n",
    "cluster by convolving it with a Gaussian kernel or a boxcar window (running average)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af97c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "SMOOTH_KERNEL = 20\n",
    "\n",
    "# Convert predictions to one-hot encoding\n",
    "one_hot = np.zeros((len(times), N_CLUSTERS + 1))\n",
    "one_hot[np.arange(len(times)), predictions] = 1\n",
    "\n",
    "# Plot the results\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "\n",
    "# Plot each cluster as a separate line\n",
    "for i in range(N_CLUSTERS):\n",
    "\n",
    "    # Obtain the detection rate by convolving with a boxcar kernel\n",
    "    detection_rate = np.convolve(one_hot[:, i], np.ones(SMOOTH_KERNEL), mode=\"same\") / SMOOTH_KERNEL\n",
    "\n",
    "    # Plot the detection rate\n",
    "    ax.plot(times, one_hot[:, i] + i, alpha=0.5)\n",
    "    ax.plot(times, detection_rate + i, color=\"black\")\n",
    "\n",
    "# Labels\n",
    "ax.set_xlabel(\"Time\")\n",
    "ax.set_ylabel(\"Cluster index\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b7f1b64e",
   "metadata": {},
   "source": [
    "### Get cluster coordinates in the feature space\n",
    "\n",
    "We here see that cluster 4 is moslty constrained by the feature 6. Looking back at the notebook 4 on the reconstruction can therefore give insights on the nature of the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce87bb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids = model.cluster_centers_\n",
    "\n",
    "# Plot the centroids\n",
    "fig = plt.figure()\n",
    "ax = plt.axes()\n",
    "\n",
    "# Show the centroids as a heatmap\n",
    "mappable = ax.matshow(centroids.T, cmap=\"Reds\")\n",
    "\n",
    "# Labels\n",
    "plt.colorbar(mappable).set_label(\"Amplitude\")\n",
    "ax.set_xlabel(\"Cluster index\")\n",
    "ax.set_ylabel(\"Feature index\")\n",
    "\n",
    "# Show\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "aa5a52f2",
   "metadata": {},
   "source": [
    "## Vizualiaze within-cluster waveforms\n",
    "\n",
    "Another way of vizualizing the clustering results is to extract the waveforms corresponding to each cluster. We can then plot the most $N$ representative waveforms for each cluster.\n",
    "\n",
    "### Extract waveforms\n",
    "\n",
    "We here extract the five most representative waveforms within each cluster. We narrow the focus on a single component of the waveforms, but the cell can be adapted to show all components at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61278419",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_WAVEFORMS = 5\n",
    "\n",
    "# Read the stream\n",
    "stream = obspy.read(\"../example/scattering_stream.mseed\").select(channel=\"BHN\")\n",
    "waveform_duration = network.bins / network.sampling_rate\n",
    "\n",
    "# Extract waveforms\n",
    "waveforms = list()\n",
    "for cluster in np.unique(predictions):\n",
    "\n",
    "    # Calculate the distance of each sample to the cluster mean\n",
    "    mean = np.mean(features[predictions == cluster], axis=0)\n",
    "    distance = np.linalg.norm(features[predictions == cluster] - mean, axis=1)\n",
    "    closest = times[predictions == cluster][distance.argsort()[:5]]\n",
    "\n",
    "    # Collect closest waveforms in a list\n",
    "    traces = list()\n",
    "    for time in closest[:N_WAVEFORMS]:\n",
    "        time = obspy.UTCDateTime(time)\n",
    "        trace = stream.slice(time, time + waveform_duration)[0].copy() \n",
    "        traces.append(trace)\n",
    "    waveforms.append(traces)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2a60c9af",
   "metadata": {},
   "source": [
    "### Plot waveforms\n",
    "\n",
    "We now plot the waveforms. We plot the five most representative waveforms for each cluster in separate axes with equal limits for comparison. Note that the very same stream that was used to infer the features and clusters are shown here. This can be changed to the spectral area where the clusters are most distinct.\n",
    "\n",
    "We clearly see that cluster 4 has more energy that the neighboring clusters. This is a good example of how the clustering algorithm can be used to extract information from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa483b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results\n",
    "fig, ax = plt.subplots(N_WAVEFORMS, N_CLUSTERS, sharex=True, sharey=True)\n",
    "\n",
    "# Plot each cluster as a separate line\n",
    "for i, traces in enumerate(waveforms):\n",
    "    ax[0, i].set_title(f\"Cluster {i}\", rotation=\"vertical\")\n",
    "    for j, trace in enumerate(traces):\n",
    "        ax[j, i].plot(trace.times(), trace.data, rasterized=True, lw=0.6, color=f\"C{i}\")\n",
    "        ax[j, i].set_axis_off()\n",
    "\n",
    "# Show\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a256b175",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "calcite",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "5d149311e5c9f98e1221145750d886da460c4fddffca98ab3723347113545849"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
